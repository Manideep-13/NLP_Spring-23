{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lda\n",
    "import lda\n",
    "\n",
    "# Gsdmm\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "from pprint import pprint\n",
    "\n",
    "# gsdmm\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    directory = 'comments1k'\n",
    "    comments = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename)) as file:\n",
    "                comments.append(file.read().strip())\n",
    "                filenames.append(filename)\n",
    "    df = pd.DataFrame({'Filename': filenames, 'comments': comments})\n",
    "\n",
    "    df.head()\n",
    "    df['comments'] = df['comments'].str.replace('&\\w+;',\" \")\n",
    "    df['comments'] = df['comments'].apply(lambda x: re.sub('<.*?>','', x))\n",
    "\n",
    "    df['comments'] = df['comments'].str.lower()\n",
    "\n",
    "    df['comments'] = df['comments'].str.replace('[^\\w\\s]',' ')\n",
    "    # Load the stop words from NLTK\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_1(df):\n",
    "    docs = df['comments'].values.tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(docs)\n",
    "    x = bag_of_words.toarray()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df):\n",
    "    docs = df['comments'].values.tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    new_vocab = vectorizer.fit(docs).get_feature_names_out()\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(df):\n",
    "    import lda\n",
    "    df1 = df\n",
    "    matrix = matrix_1(df)\n",
    "    vocab = create_vocab(df)\n",
    "    titles = df['comments'].tolist()\n",
    "    model = lda.LDA(n_topics=10, n_iter=1500, random_state=1)\n",
    "    model.fit(matrix) \n",
    "    topic_word = model.components_\n",
    "    n_top_words = 8\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    doc_topic = model.doc_topic_\n",
    "    data = []\n",
    "    docs = df.shape[0]\n",
    "    for i in range(docs):\n",
    "        filename = df1['Filename'][i]\n",
    "        title = titles[i]\n",
    "        top_topic = doc_topic[i].argmax()\n",
    "        data.append({'filename': filename,'Title': title, 'Top Topic': top_topic})\n",
    "        final_df= pd.DataFrame(data)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 1.1 - Use  Latent  Dirichlet  Allocation  (LDA)  method  to  discover  latent  topics  in  the  dataset  with  the number  of  topics  as  10.  Output  the  top  8  words  for  each  topic.  For  the  document  “0_9.txt”  and “1_7.txt”, what topics are assigned to them? Do they make sense?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saima_x4lzx52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:lda:n_documents: 996\n",
      "INFO:lda:vocab_size: 15901\n",
      "INFO:lda:n_words: 121324\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1439176\n",
      "INFO:lda:<10> log likelihood: -1163684\n",
      "INFO:lda:<20> log likelihood: -1138386\n",
      "INFO:lda:<30> log likelihood: -1125362\n",
      "INFO:lda:<40> log likelihood: -1116881\n",
      "INFO:lda:<50> log likelihood: -1111641\n",
      "INFO:lda:<60> log likelihood: -1107484\n",
      "INFO:lda:<70> log likelihood: -1103336\n",
      "INFO:lda:<80> log likelihood: -1100923\n",
      "INFO:lda:<90> log likelihood: -1097638\n",
      "INFO:lda:<100> log likelihood: -1095897\n",
      "INFO:lda:<110> log likelihood: -1093709\n",
      "INFO:lda:<120> log likelihood: -1091735\n",
      "INFO:lda:<130> log likelihood: -1090065\n",
      "INFO:lda:<140> log likelihood: -1087694\n",
      "INFO:lda:<150> log likelihood: -1087355\n",
      "INFO:lda:<160> log likelihood: -1085475\n",
      "INFO:lda:<170> log likelihood: -1083549\n",
      "INFO:lda:<180> log likelihood: -1082803\n",
      "INFO:lda:<190> log likelihood: -1081206\n",
      "INFO:lda:<200> log likelihood: -1079917\n",
      "INFO:lda:<210> log likelihood: -1079104\n",
      "INFO:lda:<220> log likelihood: -1078540\n",
      "INFO:lda:<230> log likelihood: -1076797\n",
      "INFO:lda:<240> log likelihood: -1076738\n",
      "INFO:lda:<250> log likelihood: -1075786\n",
      "INFO:lda:<260> log likelihood: -1075016\n",
      "INFO:lda:<270> log likelihood: -1074016\n",
      "INFO:lda:<280> log likelihood: -1074067\n",
      "INFO:lda:<290> log likelihood: -1073245\n",
      "INFO:lda:<300> log likelihood: -1072094\n",
      "INFO:lda:<310> log likelihood: -1072620\n",
      "INFO:lda:<320> log likelihood: -1071517\n",
      "INFO:lda:<330> log likelihood: -1071474\n",
      "INFO:lda:<340> log likelihood: -1071982\n",
      "INFO:lda:<350> log likelihood: -1071262\n",
      "INFO:lda:<360> log likelihood: -1071067\n",
      "INFO:lda:<370> log likelihood: -1070996\n",
      "INFO:lda:<380> log likelihood: -1071421\n",
      "INFO:lda:<390> log likelihood: -1070239\n",
      "INFO:lda:<400> log likelihood: -1070005\n",
      "INFO:lda:<410> log likelihood: -1069335\n",
      "INFO:lda:<420> log likelihood: -1069678\n",
      "INFO:lda:<430> log likelihood: -1068764\n",
      "INFO:lda:<440> log likelihood: -1068907\n",
      "INFO:lda:<450> log likelihood: -1068737\n",
      "INFO:lda:<460> log likelihood: -1068766\n",
      "INFO:lda:<470> log likelihood: -1068908\n",
      "INFO:lda:<480> log likelihood: -1068408\n",
      "INFO:lda:<490> log likelihood: -1067698\n",
      "INFO:lda:<500> log likelihood: -1067212\n",
      "INFO:lda:<510> log likelihood: -1066807\n",
      "INFO:lda:<520> log likelihood: -1066101\n",
      "INFO:lda:<530> log likelihood: -1066276\n",
      "INFO:lda:<540> log likelihood: -1065602\n",
      "INFO:lda:<550> log likelihood: -1066296\n",
      "INFO:lda:<560> log likelihood: -1066151\n",
      "INFO:lda:<570> log likelihood: -1066265\n",
      "INFO:lda:<580> log likelihood: -1065822\n",
      "INFO:lda:<590> log likelihood: -1065354\n",
      "INFO:lda:<600> log likelihood: -1065673\n",
      "INFO:lda:<610> log likelihood: -1065428\n",
      "INFO:lda:<620> log likelihood: -1065661\n",
      "INFO:lda:<630> log likelihood: -1065510\n",
      "INFO:lda:<640> log likelihood: -1066067\n",
      "INFO:lda:<650> log likelihood: -1065222\n",
      "INFO:lda:<660> log likelihood: -1064638\n",
      "INFO:lda:<670> log likelihood: -1065069\n",
      "INFO:lda:<680> log likelihood: -1064106\n",
      "INFO:lda:<690> log likelihood: -1063943\n",
      "INFO:lda:<700> log likelihood: -1064723\n",
      "INFO:lda:<710> log likelihood: -1063575\n",
      "INFO:lda:<720> log likelihood: -1063417\n",
      "INFO:lda:<730> log likelihood: -1063683\n",
      "INFO:lda:<740> log likelihood: -1063523\n",
      "INFO:lda:<750> log likelihood: -1063071\n",
      "INFO:lda:<760> log likelihood: -1062974\n",
      "INFO:lda:<770> log likelihood: -1063114\n",
      "INFO:lda:<780> log likelihood: -1063109\n",
      "INFO:lda:<790> log likelihood: -1062822\n",
      "INFO:lda:<800> log likelihood: -1062302\n",
      "INFO:lda:<810> log likelihood: -1062977\n",
      "INFO:lda:<820> log likelihood: -1062589\n",
      "INFO:lda:<830> log likelihood: -1062948\n",
      "INFO:lda:<840> log likelihood: -1062610\n",
      "INFO:lda:<850> log likelihood: -1062163\n",
      "INFO:lda:<860> log likelihood: -1062319\n",
      "INFO:lda:<870> log likelihood: -1062275\n",
      "INFO:lda:<880> log likelihood: -1062499\n",
      "INFO:lda:<890> log likelihood: -1062112\n",
      "INFO:lda:<900> log likelihood: -1062031\n",
      "INFO:lda:<910> log likelihood: -1061938\n",
      "INFO:lda:<920> log likelihood: -1062485\n",
      "INFO:lda:<930> log likelihood: -1062448\n",
      "INFO:lda:<940> log likelihood: -1061701\n",
      "INFO:lda:<950> log likelihood: -1062131\n",
      "INFO:lda:<960> log likelihood: -1061754\n",
      "INFO:lda:<970> log likelihood: -1062017\n",
      "INFO:lda:<980> log likelihood: -1062236\n",
      "INFO:lda:<990> log likelihood: -1061925\n",
      "INFO:lda:<1000> log likelihood: -1062443\n",
      "INFO:lda:<1010> log likelihood: -1062213\n",
      "INFO:lda:<1020> log likelihood: -1061624\n",
      "INFO:lda:<1030> log likelihood: -1062160\n",
      "INFO:lda:<1040> log likelihood: -1061052\n",
      "INFO:lda:<1050> log likelihood: -1061532\n",
      "INFO:lda:<1060> log likelihood: -1062170\n",
      "INFO:lda:<1070> log likelihood: -1061889\n",
      "INFO:lda:<1080> log likelihood: -1061743\n",
      "INFO:lda:<1090> log likelihood: -1061517\n",
      "INFO:lda:<1100> log likelihood: -1061235\n",
      "INFO:lda:<1110> log likelihood: -1061681\n",
      "INFO:lda:<1120> log likelihood: -1062032\n",
      "INFO:lda:<1130> log likelihood: -1061347\n",
      "INFO:lda:<1140> log likelihood: -1060862\n",
      "INFO:lda:<1150> log likelihood: -1060986\n",
      "INFO:lda:<1160> log likelihood: -1061145\n",
      "INFO:lda:<1170> log likelihood: -1060742\n",
      "INFO:lda:<1180> log likelihood: -1060640\n",
      "INFO:lda:<1190> log likelihood: -1060935\n",
      "INFO:lda:<1200> log likelihood: -1060756\n",
      "INFO:lda:<1210> log likelihood: -1061058\n",
      "INFO:lda:<1220> log likelihood: -1061034\n",
      "INFO:lda:<1230> log likelihood: -1061238\n",
      "INFO:lda:<1240> log likelihood: -1060856\n",
      "INFO:lda:<1250> log likelihood: -1060659\n",
      "INFO:lda:<1260> log likelihood: -1060128\n",
      "INFO:lda:<1270> log likelihood: -1060107\n",
      "INFO:lda:<1280> log likelihood: -1061199\n",
      "INFO:lda:<1290> log likelihood: -1060200\n",
      "INFO:lda:<1300> log likelihood: -1060742\n",
      "INFO:lda:<1310> log likelihood: -1060856\n",
      "INFO:lda:<1320> log likelihood: -1060910\n",
      "INFO:lda:<1330> log likelihood: -1060407\n",
      "INFO:lda:<1340> log likelihood: -1060358\n",
      "INFO:lda:<1350> log likelihood: -1060188\n",
      "INFO:lda:<1360> log likelihood: -1060396\n",
      "INFO:lda:<1370> log likelihood: -1060683\n",
      "INFO:lda:<1380> log likelihood: -1060273\n",
      "INFO:lda:<1390> log likelihood: -1060025\n",
      "INFO:lda:<1400> log likelihood: -1060458\n",
      "INFO:lda:<1410> log likelihood: -1060496\n",
      "INFO:lda:<1420> log likelihood: -1060336\n",
      "INFO:lda:<1430> log likelihood: -1060562\n",
      "INFO:lda:<1440> log likelihood: -1060103\n",
      "INFO:lda:<1450> log likelihood: -1060521\n",
      "INFO:lda:<1460> log likelihood: -1059861\n",
      "INFO:lda:<1470> log likelihood: -1059968\n",
      "INFO:lda:<1480> log likelihood: -1060543\n",
      "INFO:lda:<1490> log likelihood: -1060326\n",
      "INFO:lda:<1499> log likelihood: -1059940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: brosnan man david robert life brother river fantasy\n",
      "Topic 1: stewart jeff ned james gannon kelly western john\n",
      "Topic 2: film one story two life man well character\n",
      "Topic 3: war world young miike yokai kids film school\n",
      "Topic 4: game carla chess paul french luzhin alexandre read\n",
      "Topic 5: star series show luke wars episode new battle\n",
      "Topic 6: school high ramones matthau burns rock best comedy\n",
      "Topic 7: christmas scrooge one scott von version europa trier\n",
      "Topic 8: movie one like good see film great really\n",
      "Topic 9: davies great show comedy people marion star price\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing()\n",
    "X = lda(df)\n",
    "X.to_csv('output/lda.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Title</th>\n",
       "      <th>Top Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>bromwell high cartoon comedy ran time programs...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_7.txt</td>\n",
       "      <td>scott bartlett offon nine minutes pure crazine...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101_8.txt</td>\n",
       "      <td>imdb lists 1972 reason sources seen including ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102_10.txt</td>\n",
       "      <td>first heard film 20 years ago kid grade school...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103_7.txt</td>\n",
       "      <td>read comment decided watch movie first cast sp...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>997_7.txt</td>\n",
       "      <td>agree posts comedy drama leaned little much to...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>998_7.txt</td>\n",
       "      <td>really interesting movie action movie comedy m...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>999_10.txt</td>\n",
       "      <td>amazed movie others average 5 stars lower crap...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>99_8.txt</td>\n",
       "      <td>christmas together actually came time raised j...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9_7.txt</td>\n",
       "      <td>working class romantic drama director martin r...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                              Title  Top Topic\n",
       "0       0_9.txt  bromwell high cartoon comedy ran time programs...          6\n",
       "1     100_7.txt  scott bartlett offon nine minutes pure crazine...          2\n",
       "2     101_8.txt  imdb lists 1972 reason sources seen including ...          8\n",
       "3    102_10.txt  first heard film 20 years ago kid grade school...          8\n",
       "4     103_7.txt  read comment decided watch movie first cast sp...          8\n",
       "..          ...                                                ...        ...\n",
       "991   997_7.txt  agree posts comedy drama leaned little much to...          8\n",
       "992   998_7.txt  really interesting movie action movie comedy m...          8\n",
       "993  999_10.txt  amazed movie others average 5 stars lower crap...          8\n",
       "994    99_8.txt  christmas together actually came time raised j...          8\n",
       "995     9_7.txt  working class romantic drama director martin r...          8\n",
       "\n",
       "[996 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename                                               0_9.txt\n",
       "Title        bromwell high cartoon comedy ran time programs...\n",
       "Top Topic                                                    6\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename                                               1_7.txt\n",
       "Title        like adult comedy cartoons like south park nea...\n",
       "Top Topic                                                    8\n",
       "Name: 111, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic 0 is about Pierce Brosnan because it contains words related to his life and career such as \"man\", \"david\", \"robert\", \"life\", \"brother\", \"river\" It seems. It also contains words related to his imaginary life, such as \"fantasy\" and \"dream\".\n",
    "- Topic 1 seems to be related to actors because it contains words related to the career of actors such as \"movie\", \"one\", \"story\", \"two\", \"life\", \"man\", and \"person\". Also included are words related to her personal life, such as Jeff, Ned, James, Kelly, Western, and John.\n",
    "- Topic 2 appears to be about movies because it contains words related to making movies, such as \"movie,\" \"one,\" \"story,\" and \"two.\" It also includes words related to personality and behavior, such as \"man,\" \"nice,\" and \"personality.\"\n",
    "- Topic 3 seems to be related to Japanese culture because it contains words related to movies such as \"miike\", \"yokai\", \"kids\", \"film\", and \"school\". It also includes words related to her game of chess, such as \"carla\", \"chess\", \"paul\" and \"french\".\n",
    "- Topic 4 appears to be about chess because it contains game-related words such as \"carla\", \"chess\", \"paul\", and \"french\". It also contains words related to athletes, such as \"Luzin\" and \"Alexandre\". \n",
    "- Topic 5 seems to be centered around the Star Wars series, as it includes words related to characters and plots such as \"Luke,\" \"Wars,\" \"Episode,\" and \"Battle.\"\n",
    "- Topic 6 seems to revolve around high school, as it contains words related to the experience such as \"Ramones\", \"Mathaw\", \"Burns\", \"Rock\", and \"Best\". There are also words related to students such as \"comedy\" and \"marion\".\n",
    "- Topic 7 seems to center around the movie A Christmas Carol as it contains words related to the story such as 'Scrooge', 'One', 'Scott', 'Von' and 'Europa'. I can see it. Words related to characters such as \"Christmas\" and \"ghost\" are also included. \n",
    "- Topic 8 seems to be about movies because it contains words about movie quality, such as \"1\", \"like\", \"good\", and \"watch\". It also includes words related to entertainment, such as \"movie\", \"great\", and \"really\".\n",
    "- Topic 9 appears to be centered around the comedy show Davies, as it contains words related to the show such as \"great\", \"show\", \"comedy\", \"people\", \"Marion\", and \"star\". looks like And so on, words related to the cast are also included. B. \"Price\". \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2: Because of the data sparsity, short text may not  provide  enough  context  to  adequately  inform  topic modeling.Try  Biterm,  GSDMM  or  other  short  text  topic  model  for  our  dataset.  Compare  the  topic modelling results with LDA, any improvement?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saima_x4lzx52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 888 clusters with 3 clusters populated\n",
      "In stage 1: transferred 12 clusters with 2 clusters populated\n",
      "In stage 2: transferred 0 clusters with 2 clusters populated\n",
      "In stage 3: transferred 0 clusters with 2 clusters populated\n",
      "In stage 4: transferred 0 clusters with 2 clusters populated\n",
      "Number of documents per topic : [702   0   0   0   0   0   0   0   0 294]\n",
      "Most important clusters (by number of docs inside): [0 9 8 7 6 5 4 3 2 1]\n",
      "\n",
      "Cluster 0 : [(0, 11118343), (1, 39049), (2, 3847), (3, 829), (4, 271), (5, 86), (6, 35), (7, 26), (8, 8), (9, 5), (11, 2), (10, 1)]\n",
      "\n",
      "Cluster 9 : [(0, 4620527), (1, 45284), (2, 6027), (3, 1687), (4, 615), (5, 299), (6, 171), (7, 93), (8, 65), (9, 36), (10, 27), (11, 18), (13, 10), (12, 8), (14, 5), (15, 4), (17, 4), (18, 3), (20, 3), (16, 2)]\n",
      "\n",
      "Cluster 8 : []\n",
      "\n",
      "Cluster 7 : []\n",
      "\n",
      "Cluster 6 : []\n",
      "\n",
      "Cluster 5 : []\n",
      "\n",
      "Cluster 4 : []\n",
      "\n",
      "Cluster 3 : []\n",
      "\n",
      "Cluster 2 : []\n",
      "\n",
      "Cluster 1 : []\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[39m=\u001b[39m preprocessing()\n\u001b[0;32m      2\u001b[0m Y \u001b[39m=\u001b[39m gsdmm(df)\n\u001b[1;32m----> 3\u001b[0m Y\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39m\u001b[39moutput/gsdmm.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "df = preprocessing()\n",
    "Y = biterm(df)\n",
    "Y.to_csv('output/biterm.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1 When there is no (enough) labelled corpus to train a machine learning based NLP model, we need to \n",
    "create a training text dataset as golden standard through manual annotation. Choose a text annotation \n",
    "tool to finish the following two text annotation tasks: \n",
    " \n",
    "<i> Entity  Annotation:  “Barack  Obama  was  the  44th  President  of  the  United  States.  He  was  born  in \n",
    "Hawaii and studied law at Harvard University.”  \n",
    "Annotation Results:  \n",
    "      Barack Obama PERSON \n",
    "      44th CARDINAL \n",
    "      the United States GPE \n",
    "      Hawaii GPE \n",
    "      Harvard University ORG </i>\n",
    " \n",
    "<i> Sentiment Annotation: “De Niro has the ability to make every role he portrays into acting gold. He \n",
    "gives a great performance in this film and there is a great scene where he has to take his father to a \n",
    "home for elderly people because he can't care for him anymore that will break your heart.  I will say \n",
    "you won't see much bette acting anywhere.”  \n",
    "Annotation Results: Positive </i>\n",
    " </b>\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "#### For this task, I used label studio to perform annotation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label Studio: It is an open-source application for data annotation that supports a variety of annotation kinds, including object identification, named entity recognition, and text classification.\n",
    "- Source: https://github.com/heartexlabs/label-studio/\n",
    "- We can install label-studio in anaconda environment, by below steps:\n",
    "\n",
    "- step 1: conda create --name label-studio\n",
    "- step 2: conda activate label-studio\n",
    "- step 3: pip install label-studio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>After installation you can navigate to the google chrome browser and signup for a new account.\n",
    "<i> Click on the New project on landing page.\n",
    "\n",
    "<img src=\"output/2-1/1.png\"/>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> You will get below popup window and Enter the project name\n",
    "\n",
    "<img src = \"output/2-1/2.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Now Click on the Data import and import the desired text file\n",
    "\n",
    "<img src =\"output/2-1/3.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Now click on the Labeling setup and select the <b>Natural Language Processing</b> and select the <b>Named Entity Recognition</b> tab.\n",
    "\n",
    "<img src = \"output/2-1/entity.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - <i> You will be navigated to new page and Remove all the pre existing labels and <b>add desired labels</b>\n",
    " - <i> Select the Configure date as import file from the dropdown.\n",
    " - <i> After completion of two steps click the Save button.</i>\n",
    "\n",
    "<img src= \"output/2-1/4.png\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>After clicking on the save button, You will be redirected to the new page to labeling the text.\n",
    "<img src = \"output/2-1/new.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select the ID for the label You will be redirected to new page for labeling.\n",
    "<b>Follow this steps for labeling:\n",
    "    Select the label and highlight the desired text and click enter</b>\n",
    "\n",
    "<img src = \"output/2-1/k.png\">\n",
    "<b>Now repeat all the steps for the remaining labels</b>\n",
    "<img src = \"output/2-1/5.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat the same process for sentiment annotation for upto data importing**\n",
    "\n",
    "<img src =\"output/2-1/6.png\">\n",
    "\n",
    "<img src = 'output/2-1/7.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Now, select the labeling method, select the <b> Natural Language Processing </b> and select the <b> text classification </b>\n",
    "\n",
    "<img src= 'output/2-1/text-cla.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> After selecting, you will be redirected to the next page. </i> \n",
    "<i> Click on the <b>Save</b> button. </i>\n",
    "\n",
    "<img src=\"output/2-1/8.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> You will be redirected to the new page, Now select the ID and Select the type of the <b>sentiment</b>\n",
    "<i> Click on the <b> submit </b> button to save the annotation </i>\n",
    "\n",
    "<img src = \"output/2-1/9.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Thus, Entity and Sentiment Annotation is done by label-studio. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
