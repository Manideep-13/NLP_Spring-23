{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "\n",
    "# lda\n",
    "import lda\n",
    "\n",
    "# Gsdmm\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score \n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    directory = 'comments1k'\n",
    "    comments = []\n",
    "    filenames = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with open(os.path.join(directory, filename)) as file:\n",
    "                comments.append(file.read().strip())\n",
    "                filenames.append(filename)\n",
    "    df = pd.DataFrame({'Filename': filenames, 'comments': comments})\n",
    "\n",
    "    df.head()\n",
    "    df['comments'] = df['comments'].str.replace('&\\w+;',\" \")\n",
    "    df['comments'] = df['comments'].apply(lambda x: re.sub('<.*?>','', x))\n",
    "\n",
    "    df['comments'] = df['comments'].str.lower()\n",
    "\n",
    "    df['comments'] = df['comments'].str.replace('[^\\w\\s]',' ')\n",
    "    # Load the stop words from NLTK\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['comments'] = df['comments'].apply(lambda x: \" \".join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create N-grams\n",
    "def make_n_grams(texts):\n",
    "    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)  # higher threshold fewer phrases.\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "    bigrams_text = [bigram_mod[doc] for doc in texts]\n",
    "    trigrams_text =  [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n",
    "    return trigrams_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_1(df):\n",
    "    docs = df['comments'].values.tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    bag_of_words = vectorizer.fit_transform(docs)\n",
    "    x = bag_of_words.toarray()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(df):\n",
    "    docs = df['comments'].values.tolist()\n",
    "    vectorizer = CountVectorizer()\n",
    "    new_vocab = vectorizer.fit(docs).get_feature_names_out()\n",
    "    return new_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(df):\n",
    "    import lda\n",
    "    df1 = df\n",
    "    matrix = matrix_1(df)\n",
    "    vocab = create_vocab(df)\n",
    "    titles = df['comments'].tolist()\n",
    "    model = lda.LDA(n_topics=10, n_iter=1500, random_state=1)\n",
    "    model.fit(matrix) \n",
    "    topic_word = model.components_\n",
    "    n_top_words = 8\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ','.join(topic_words)))\n",
    "    doc_topic = model.doc_topic_\n",
    "    data = []\n",
    "    docs = df.shape[0]\n",
    "    for i in range(docs):\n",
    "        filename = df1['Filename'][i]\n",
    "        title = titles[i]\n",
    "        top_topic = doc_topic[i].argmax()\n",
    "        data.append({'filename': filename,'Title': title, 'Top Topic': top_topic})\n",
    "        final_df= pd.DataFrame(data)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gsdmm(df, reviews_lemmatized):\n",
    "    n_topics = 10\n",
    "    mgp = MovieGroupProcess(K=n_topics, alpha=0.01, beta=0.01, n_iters=2)\n",
    "\n",
    "    vocab = set(x for review in reviews_lemmatized for x in review)\n",
    "    n_terms = len(vocab)\n",
    "    model = mgp.fit(reviews_lemmatized, n_terms)\n",
    " \n",
    "    return mgp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts =sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\n Topic %s : %s\"%(cluster,sort_dicts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topics_dataframe(df,data_text,mgp, threshold, topic_dict,lemma_text):\n",
    "    result = pd.DataFrame(columns=['Filename','comments', 'Topic', 'Lemma-text'])\n",
    "    for i, text in enumerate(data_text):\n",
    "        result.at[i,'comments'] = df.comments[i]\n",
    "        result.at[i, 'Filename'] = df.Filename[i]\n",
    "        result.at[i, 'Lemma-text'] = lemma_text[i]\n",
    "        prob = mgp.choose_best_label(lemma_text[i])\n",
    "        if prob[1] >= threshold:\n",
    "            result.at[i, 'Topic'] = topic_dict[prob[0]]\n",
    "        else:\n",
    "            result.at[i, 'Topic'] = 'Other'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 1.1 - Use  Latent  Dirichlet  Allocation  (LDA)  method  to  discover  latent  topics  in  the  dataset  with  the number  of  topics  as  10.  Output  the  top  8  words  for  each  topic.  For  the  document  “0_9.txt”  and “1_7.txt”, what topics are assigned to them? Do they make sense?</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saima_x4lzx52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:lda:n_documents: 996\n",
      "INFO:lda:vocab_size: 15901\n",
      "INFO:lda:n_words: 121324\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -1439176\n",
      "INFO:lda:<10> log likelihood: -1163684\n",
      "INFO:lda:<20> log likelihood: -1138386\n",
      "INFO:lda:<30> log likelihood: -1125362\n",
      "INFO:lda:<40> log likelihood: -1116881\n",
      "INFO:lda:<50> log likelihood: -1111641\n",
      "INFO:lda:<60> log likelihood: -1107484\n",
      "INFO:lda:<70> log likelihood: -1103336\n",
      "INFO:lda:<80> log likelihood: -1100923\n",
      "INFO:lda:<90> log likelihood: -1097638\n",
      "INFO:lda:<100> log likelihood: -1095897\n",
      "INFO:lda:<110> log likelihood: -1093709\n",
      "INFO:lda:<120> log likelihood: -1091735\n",
      "INFO:lda:<130> log likelihood: -1090065\n",
      "INFO:lda:<140> log likelihood: -1087694\n",
      "INFO:lda:<150> log likelihood: -1087355\n",
      "INFO:lda:<160> log likelihood: -1085475\n",
      "INFO:lda:<170> log likelihood: -1083549\n",
      "INFO:lda:<180> log likelihood: -1082803\n",
      "INFO:lda:<190> log likelihood: -1081206\n",
      "INFO:lda:<200> log likelihood: -1079917\n",
      "INFO:lda:<210> log likelihood: -1079104\n",
      "INFO:lda:<220> log likelihood: -1078540\n",
      "INFO:lda:<230> log likelihood: -1076797\n",
      "INFO:lda:<240> log likelihood: -1076738\n",
      "INFO:lda:<250> log likelihood: -1075786\n",
      "INFO:lda:<260> log likelihood: -1075016\n",
      "INFO:lda:<270> log likelihood: -1074016\n",
      "INFO:lda:<280> log likelihood: -1074067\n",
      "INFO:lda:<290> log likelihood: -1073245\n",
      "INFO:lda:<300> log likelihood: -1072094\n",
      "INFO:lda:<310> log likelihood: -1072620\n",
      "INFO:lda:<320> log likelihood: -1071517\n",
      "INFO:lda:<330> log likelihood: -1071474\n",
      "INFO:lda:<340> log likelihood: -1071982\n",
      "INFO:lda:<350> log likelihood: -1071262\n",
      "INFO:lda:<360> log likelihood: -1071067\n",
      "INFO:lda:<370> log likelihood: -1070996\n",
      "INFO:lda:<380> log likelihood: -1071421\n",
      "INFO:lda:<390> log likelihood: -1070239\n",
      "INFO:lda:<400> log likelihood: -1070005\n",
      "INFO:lda:<410> log likelihood: -1069335\n",
      "INFO:lda:<420> log likelihood: -1069678\n",
      "INFO:lda:<430> log likelihood: -1068764\n",
      "INFO:lda:<440> log likelihood: -1068907\n",
      "INFO:lda:<450> log likelihood: -1068737\n",
      "INFO:lda:<460> log likelihood: -1068766\n",
      "INFO:lda:<470> log likelihood: -1068908\n",
      "INFO:lda:<480> log likelihood: -1068408\n",
      "INFO:lda:<490> log likelihood: -1067698\n",
      "INFO:lda:<500> log likelihood: -1067212\n",
      "INFO:lda:<510> log likelihood: -1066807\n",
      "INFO:lda:<520> log likelihood: -1066101\n",
      "INFO:lda:<530> log likelihood: -1066276\n",
      "INFO:lda:<540> log likelihood: -1065602\n",
      "INFO:lda:<550> log likelihood: -1066296\n",
      "INFO:lda:<560> log likelihood: -1066151\n",
      "INFO:lda:<570> log likelihood: -1066265\n",
      "INFO:lda:<580> log likelihood: -1065822\n",
      "INFO:lda:<590> log likelihood: -1065354\n",
      "INFO:lda:<600> log likelihood: -1065673\n",
      "INFO:lda:<610> log likelihood: -1065428\n",
      "INFO:lda:<620> log likelihood: -1065661\n",
      "INFO:lda:<630> log likelihood: -1065510\n",
      "INFO:lda:<640> log likelihood: -1066067\n",
      "INFO:lda:<650> log likelihood: -1065222\n",
      "INFO:lda:<660> log likelihood: -1064638\n",
      "INFO:lda:<670> log likelihood: -1065069\n",
      "INFO:lda:<680> log likelihood: -1064106\n",
      "INFO:lda:<690> log likelihood: -1063943\n",
      "INFO:lda:<700> log likelihood: -1064723\n",
      "INFO:lda:<710> log likelihood: -1063575\n",
      "INFO:lda:<720> log likelihood: -1063417\n",
      "INFO:lda:<730> log likelihood: -1063683\n",
      "INFO:lda:<740> log likelihood: -1063523\n",
      "INFO:lda:<750> log likelihood: -1063071\n",
      "INFO:lda:<760> log likelihood: -1062974\n",
      "INFO:lda:<770> log likelihood: -1063114\n",
      "INFO:lda:<780> log likelihood: -1063109\n",
      "INFO:lda:<790> log likelihood: -1062822\n",
      "INFO:lda:<800> log likelihood: -1062302\n",
      "INFO:lda:<810> log likelihood: -1062977\n",
      "INFO:lda:<820> log likelihood: -1062589\n",
      "INFO:lda:<830> log likelihood: -1062948\n",
      "INFO:lda:<840> log likelihood: -1062610\n",
      "INFO:lda:<850> log likelihood: -1062163\n",
      "INFO:lda:<860> log likelihood: -1062319\n",
      "INFO:lda:<870> log likelihood: -1062275\n",
      "INFO:lda:<880> log likelihood: -1062499\n",
      "INFO:lda:<890> log likelihood: -1062112\n",
      "INFO:lda:<900> log likelihood: -1062031\n",
      "INFO:lda:<910> log likelihood: -1061938\n",
      "INFO:lda:<920> log likelihood: -1062485\n",
      "INFO:lda:<930> log likelihood: -1062448\n",
      "INFO:lda:<940> log likelihood: -1061701\n",
      "INFO:lda:<950> log likelihood: -1062131\n",
      "INFO:lda:<960> log likelihood: -1061754\n",
      "INFO:lda:<970> log likelihood: -1062017\n",
      "INFO:lda:<980> log likelihood: -1062236\n",
      "INFO:lda:<990> log likelihood: -1061925\n",
      "INFO:lda:<1000> log likelihood: -1062443\n",
      "INFO:lda:<1010> log likelihood: -1062213\n",
      "INFO:lda:<1020> log likelihood: -1061624\n",
      "INFO:lda:<1030> log likelihood: -1062160\n",
      "INFO:lda:<1040> log likelihood: -1061052\n",
      "INFO:lda:<1050> log likelihood: -1061532\n",
      "INFO:lda:<1060> log likelihood: -1062170\n",
      "INFO:lda:<1070> log likelihood: -1061889\n",
      "INFO:lda:<1080> log likelihood: -1061743\n",
      "INFO:lda:<1090> log likelihood: -1061517\n",
      "INFO:lda:<1100> log likelihood: -1061235\n",
      "INFO:lda:<1110> log likelihood: -1061681\n",
      "INFO:lda:<1120> log likelihood: -1062032\n",
      "INFO:lda:<1130> log likelihood: -1061347\n",
      "INFO:lda:<1140> log likelihood: -1060862\n",
      "INFO:lda:<1150> log likelihood: -1060986\n",
      "INFO:lda:<1160> log likelihood: -1061145\n",
      "INFO:lda:<1170> log likelihood: -1060742\n",
      "INFO:lda:<1180> log likelihood: -1060640\n",
      "INFO:lda:<1190> log likelihood: -1060935\n",
      "INFO:lda:<1200> log likelihood: -1060756\n",
      "INFO:lda:<1210> log likelihood: -1061058\n",
      "INFO:lda:<1220> log likelihood: -1061034\n",
      "INFO:lda:<1230> log likelihood: -1061238\n",
      "INFO:lda:<1240> log likelihood: -1060856\n",
      "INFO:lda:<1250> log likelihood: -1060659\n",
      "INFO:lda:<1260> log likelihood: -1060128\n",
      "INFO:lda:<1270> log likelihood: -1060107\n",
      "INFO:lda:<1280> log likelihood: -1061199\n",
      "INFO:lda:<1290> log likelihood: -1060200\n",
      "INFO:lda:<1300> log likelihood: -1060742\n",
      "INFO:lda:<1310> log likelihood: -1060856\n",
      "INFO:lda:<1320> log likelihood: -1060910\n",
      "INFO:lda:<1330> log likelihood: -1060407\n",
      "INFO:lda:<1340> log likelihood: -1060358\n",
      "INFO:lda:<1350> log likelihood: -1060188\n",
      "INFO:lda:<1360> log likelihood: -1060396\n",
      "INFO:lda:<1370> log likelihood: -1060683\n",
      "INFO:lda:<1380> log likelihood: -1060273\n",
      "INFO:lda:<1390> log likelihood: -1060025\n",
      "INFO:lda:<1400> log likelihood: -1060458\n",
      "INFO:lda:<1410> log likelihood: -1060496\n",
      "INFO:lda:<1420> log likelihood: -1060336\n",
      "INFO:lda:<1430> log likelihood: -1060562\n",
      "INFO:lda:<1440> log likelihood: -1060103\n",
      "INFO:lda:<1450> log likelihood: -1060521\n",
      "INFO:lda:<1460> log likelihood: -1059861\n",
      "INFO:lda:<1470> log likelihood: -1059968\n",
      "INFO:lda:<1480> log likelihood: -1060543\n",
      "INFO:lda:<1490> log likelihood: -1060326\n",
      "INFO:lda:<1499> log likelihood: -1059940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: brosnan,man,david,robert,life,brother,river,fantasy\n",
      "Topic 1: stewart,jeff,ned,james,gannon,kelly,western,john\n",
      "Topic 2: film,one,story,two,life,man,well,character\n",
      "Topic 3: war,world,young,miike,yokai,kids,film,school\n",
      "Topic 4: game,carla,chess,paul,french,luzhin,alexandre,read\n",
      "Topic 5: star,series,show,luke,wars,episode,new,battle\n",
      "Topic 6: school,high,ramones,matthau,burns,rock,best,comedy\n",
      "Topic 7: christmas,scrooge,one,scott,von,version,europa,trier\n",
      "Topic 8: movie,one,like,good,see,film,great,really\n",
      "Topic 9: davies,great,show,comedy,people,marion,star,price\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing()\n",
    "X = lda(df)\n",
    "X.to_csv('output/lda.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>Title</th>\n",
       "      <th>Top Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>bromwell high cartoon comedy ran time programs...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_7.txt</td>\n",
       "      <td>scott bartlett offon nine minutes pure crazine...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101_8.txt</td>\n",
       "      <td>imdb lists 1972 reason sources seen including ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102_10.txt</td>\n",
       "      <td>first heard film 20 years ago kid grade school...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103_7.txt</td>\n",
       "      <td>read comment decided watch movie first cast sp...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>997_7.txt</td>\n",
       "      <td>agree posts comedy drama leaned little much to...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>998_7.txt</td>\n",
       "      <td>really interesting movie action movie comedy m...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>999_10.txt</td>\n",
       "      <td>amazed movie others average 5 stars lower crap...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>99_8.txt</td>\n",
       "      <td>christmas together actually came time raised j...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9_7.txt</td>\n",
       "      <td>working class romantic drama director martin r...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       filename                                              Title  Top Topic\n",
       "0       0_9.txt  bromwell high cartoon comedy ran time programs...          6\n",
       "1     100_7.txt  scott bartlett offon nine minutes pure crazine...          2\n",
       "2     101_8.txt  imdb lists 1972 reason sources seen including ...          8\n",
       "3    102_10.txt  first heard film 20 years ago kid grade school...          8\n",
       "4     103_7.txt  read comment decided watch movie first cast sp...          8\n",
       "..          ...                                                ...        ...\n",
       "991   997_7.txt  agree posts comedy drama leaned little much to...          8\n",
       "992   998_7.txt  really interesting movie action movie comedy m...          8\n",
       "993  999_10.txt  amazed movie others average 5 stars lower crap...          8\n",
       "994    99_8.txt  christmas together actually came time raised j...          8\n",
       "995     9_7.txt  working class romantic drama director martin r...          8\n",
       "\n",
       "[996 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename                                               0_9.txt\n",
       "Title        bromwell high cartoon comedy ran time programs...\n",
       "Top Topic                                                    6\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename                                               1_7.txt\n",
       "Title        like adult comedy cartoons like south park nea...\n",
       "Top Topic                                                    8\n",
       "Name: 111, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[111]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic 0: It seems that this topic is related to movies or TV shows that feature actors like Brosnan, David, and Robert, as well as words like \"life,\" \"brother,\" and \"fantasy.\" This topic may be discussed in reference to fantasies involving characters played by these actors or movies or TV shows that center on familial bonds, particularly brotherhood.\n",
    "\n",
    "- Topic 1: This topic appears to be related to western films or television programs because it contains the phrases \"Stewart,\" \"Jeff,\" \"Ned,\" \"James,\" \"Gannon,\" \"Kelly,\" and \"John.\" This subject could involve discussing particular western films or TV episodes, or it could explore the themes, characters, and cliches more broadly.\n",
    "\n",
    "- Topic 2: This topic is broader and covers a variety of movies and TV shows that use the phrases \"film,\" \"story,\" \"life,\", \"man,\",\"well,\" and \"character.\" Without more background, it is challenging to determine the topic's precise focus.\n",
    "\n",
    "- Topic 3: Words like \"world,\" \"young,\" \"miike,\" \"yokai,\" \"kids,\" \"film,\" and \"school\" appear to be references to war films or TV series that feature young characters. It's possible that this subject will examine how war is portrayed in films and television shows, particularly from the viewpoint of characters who are younger.\n",
    "\n",
    "- Topic 4 seems to be about chess because it includes words like \"carla,\" \"chess,\" \"paul,\" and \"french,\" which are game-related. It also includes nouns with athletic connotations like \"Luzin\" and \"Alexandre.\"\n",
    "\n",
    "- Topic 5 appears to be focused on the Star Wars series because it contains phrases like \"Luke,\" \"Wars,\" \"Episode,\" and \"Battle\" that are associated with characters and stories.\n",
    "\n",
    "- Topic 6 appears to be about high school because it has phrases like \"Ramones,\" \"Mathaw,\" \"Burns,\" \"Rock,\" and \"Best\" that are associated with the time period. Students are also mentioned in words like \"comedy\" and \"marion.\"\n",
    "\n",
    "- Topic 7 appears to be focused on the film A Christmas Carol because it includes phrases like \"Scrooge,\" \"One,\" \"Scott,\" \"Von,\" and \"Europa.\" I recognize it. Character-related words like \"Christmas\" and \"ghost\" are also present.\n",
    "\n",
    "\n",
    "- Topic 8 appears to be about movies because it includes terms like \"1,\" \"like,\" \"good,\" and \"watch\" that refer to movie quality. It also contains phrases like \"movie,\" \"amazing,\" and \"truly\" that are associated with entertainment.\n",
    "\n",
    "- Topic 9 appears to be focused on the comedy series Davies because it contains phrases like \"excellent\", \"show\", \"comedy\", \"people\", \"Marion\", and \"star\" that are associated with the program. seems like Words pertaining to the cast are also included, and so on. \"Price\" in B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1.2: Because of the data sparsity, short text may not  provide  enough  context  to  adequately  inform  topic modeling.Try  Biterm,  GSDMM  or  other  short  text  topic  model  for  our  dataset.  Compare  the  topic modelling results with LDA, any improvement?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using GSDMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>bromwell high cartoon comedy ran time programs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_7.txt</td>\n",
       "      <td>scott bartlett offon nine minutes pure crazine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101_8.txt</td>\n",
       "      <td>imdb lists 1972 reason sources seen including ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102_10.txt</td>\n",
       "      <td>first heard film 20 years ago kid grade school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103_7.txt</td>\n",
       "      <td>read comment decided watch movie first cast sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>997_7.txt</td>\n",
       "      <td>agree posts comedy drama leaned little much to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>998_7.txt</td>\n",
       "      <td>really interesting movie action movie comedy m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>999_10.txt</td>\n",
       "      <td>amazed movie others average 5 stars lower crap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>99_8.txt</td>\n",
       "      <td>christmas together actually came time raised j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9_7.txt</td>\n",
       "      <td>working class romantic drama director martin r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename                                           comments\n",
       "0       0_9.txt  bromwell high cartoon comedy ran time programs...\n",
       "1     100_7.txt  scott bartlett offon nine minutes pure crazine...\n",
       "2     101_8.txt  imdb lists 1972 reason sources seen including ...\n",
       "3    102_10.txt  first heard film 20 years ago kid grade school...\n",
       "4     103_7.txt  read comment decided watch movie first cast sp...\n",
       "..          ...                                                ...\n",
       "991   997_7.txt  agree posts comedy drama leaned little much to...\n",
       "992   998_7.txt  really interesting movie action movie comedy m...\n",
       "993  999_10.txt  amazed movie others average 5 stars lower crap...\n",
       "994    99_8.txt  christmas together actually came time raised j...\n",
       "995     9_7.txt  working class romantic drama director martin r...\n",
       "\n",
       "[996 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saima_x4lzx52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:gensim.models.phrases:collecting all words and their counts\n",
      "INFO:gensim.models.phrases:PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO:gensim.models.phrases:collected 115874 token types (unigram + bigrams) from a corpus of 120425 words and 996 sentences\n",
      "INFO:gensim.models.phrases:merged Phrases<115874 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "INFO:gensim.utils:Phrases lifecycle event {'msg': 'built Phrases<115874 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.18s', 'datetime': '2023-03-25T00:09:38.396928', 'gensim': '4.3.1', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "INFO:gensim.models.phrases:exporting phrases from Phrases<115874 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "INFO:gensim.utils:FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<297 phrases, min_count=5, threshold=100> from Phrases<115874 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.24s', 'datetime': '2023-03-25T00:09:38.637833', 'gensim': '4.3.1', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "INFO:gensim.models.phrases:collecting all words and their counts\n",
      "INFO:gensim.models.phrases:PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "INFO:gensim.models.phrases:collected 116120 token types (unigram + bigrams) from a corpus of 117088 words and 996 sentences\n",
      "INFO:gensim.models.phrases:merged Phrases<116120 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "INFO:gensim.utils:Phrases lifecycle event {'msg': 'built Phrases<116120 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.40s', 'datetime': '2023-03-25T00:09:39.048875', 'gensim': '4.3.1', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "INFO:gensim.models.phrases:exporting phrases from Phrases<116120 vocab, min_count=5, threshold=100, max_vocab_size=40000000>\n",
      "INFO:gensim.utils:FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<246 phrases, min_count=5, threshold=100> from Phrases<116120 vocab, min_count=5, threshold=100, max_vocab_size=40000000> in 0.20s', 'datetime': '2023-03-25T00:09:39.250896', 'gensim': '4.3.1', 'python': '3.10.6 (tags/v3.10.6:9c7b4bd, Aug  1 2022, 21:53:49) [MSC v.1932 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In stage 0: transferred 865 clusters with 10 clusters populated\n",
      "In stage 1: transferred 365 clusters with 10 clusters populated\n",
      "Number of documents per topic : [  7  19  19   8   2   7   2   3   2 927]\n",
      "\n",
      "Most important clusters (by number of docs inside): [9 2 1 3 5 0 7 8 6 4]\n",
      "\n",
      " Topic 9 : [('film', 1887), ('movie', 1731), ('see', 943), ('make', 795), ('story', 615), ('time', 610), ('character', 543), ('well', 536)]\n",
      "\n",
      " Topic 2 : [('movie', 41), ('make', 15), ('watch', 13), ('time', 13), ('see', 12), ('know', 12), ('well', 11), ('love', 11)]\n",
      "\n",
      " Topic 1 : [('movie', 33), ('see', 20), ('film', 18), ('think', 17), ('find', 13), ('show', 12), ('time', 11), ('make', 11)]\n",
      "\n",
      " Topic 3 : [('movie', 12), ('work', 9), ('crawford', 9), ('make', 8), ('even', 5), ('see', 5), ('dance', 5), ('film', 4)]\n",
      "\n",
      " Topic 5 : [('movie', 10), ('watch', 7), ('stooge', 5), ('shemp', 5), ('give', 3), ('really', 3), ('entertain', 3), ('short', 3)]\n",
      "\n",
      " Topic 0 : [('get', 12), ('movie', 10), ('go', 10), ('friend', 7), ('love', 6), ('make', 6), ('see', 5), ('character', 5)]\n",
      "\n",
      " Topic 7 : [('see', 8), ('movie', 3), ('character', 2), ('love', 2), ('film', 2), ('look', 2), ('time', 2), ('show', 2)]\n",
      "\n",
      " Topic 8 : [('movie', 4), ('say', 4), ('ramone', 3), ('watch', 2), ('know', 2), ('rock_roll_high_school', 2), ('case', 1), ('get', 1)]\n",
      "\n",
      " Topic 6 : [('think', 11), ('star', 4), ('movie', 3), ('make', 3), ('davy', 3), ('film', 2), ('feel', 2), ('act', 2)]\n",
      "\n",
      " Topic 4 : [('kid', 6), ('miike', 6), ('much', 4), ('film', 4), ('pretty', 4), ('fun', 4), ('bit', 4), ('time', 2)]\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing()\n",
    "tokens_reviews = list(sent_to_words(df['comments']))\n",
    "tokens_reviews = make_n_grams(tokens_reviews)\n",
    "reviews_lemmatized = lemmatization(tokens_reviews, allowed_postags=['NOUN', 'VERB', 'ADV'])\n",
    "mgp = gsdmm(df,reviews_lemmatized)\n",
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-10:][::-1]\n",
    "print('\\nMost important clusters (by number of docs inside):', top_index)\n",
    "topic_dict = {}\n",
    "topic_names = [1,2,3,4,5,6,7,8,9,10]\n",
    "for i, topic_num in enumerate(top_index):\n",
    "    topic_dict[topic_num]=topic_names[i] \n",
    "# show the top 5 words in term frequency for each cluster \n",
    "top_words(mgp.cluster_word_distribution, top_index, 8)\n",
    "result = create_topics_dataframe(df,data_text=df.comments, mgp=mgp, threshold=0.3, topic_dict=topic_dict, lemma_text=reviews_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>comments</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Lemma-text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_9.txt</td>\n",
       "      <td>bromwell high cartoon comedy ran time programs...</td>\n",
       "      <td>1</td>\n",
       "      <td>[comedy, run, time, school, life, teacher, yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100_7.txt</td>\n",
       "      <td>scott bartlett offon nine minutes pure crazine...</td>\n",
       "      <td>Other</td>\n",
       "      <td>[minute, craziness, assault, psychedelic, puls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101_8.txt</td>\n",
       "      <td>imdb lists 1972 reason sources seen including ...</td>\n",
       "      <td>4</td>\n",
       "      <td>[list, reason, source, see, include, program, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>102_10.txt</td>\n",
       "      <td>first heard film 20 years ago kid grade school...</td>\n",
       "      <td>1</td>\n",
       "      <td>[first, hear, film, kid, grade, school, happen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>103_7.txt</td>\n",
       "      <td>read comment decided watch movie first cast sp...</td>\n",
       "      <td>3</td>\n",
       "      <td>[read, comment, decide, watch, movie, first, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>997_7.txt</td>\n",
       "      <td>agree posts comedy drama leaned little much to...</td>\n",
       "      <td>1</td>\n",
       "      <td>[agree, post, comedy, drama, lean, much, comed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>998_7.txt</td>\n",
       "      <td>really interesting movie action movie comedy m...</td>\n",
       "      <td>1</td>\n",
       "      <td>[really, movie, action, movie, comedy, foxx, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>999_10.txt</td>\n",
       "      <td>amazed movie others average 5 stars lower crap...</td>\n",
       "      <td>Other</td>\n",
       "      <td>[movie, other, star, lower, movie, average, st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>99_8.txt</td>\n",
       "      <td>christmas together actually came time raised j...</td>\n",
       "      <td>1</td>\n",
       "      <td>[together, actually, come, time, raise, song, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>9_7.txt</td>\n",
       "      <td>working class romantic drama director martin r...</td>\n",
       "      <td>1</td>\n",
       "      <td>[work, class, drama, director, ritt, come, yet...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Filename                                           comments  Topic  \\\n",
       "0       0_9.txt  bromwell high cartoon comedy ran time programs...      1   \n",
       "1     100_7.txt  scott bartlett offon nine minutes pure crazine...  Other   \n",
       "2     101_8.txt  imdb lists 1972 reason sources seen including ...      4   \n",
       "3    102_10.txt  first heard film 20 years ago kid grade school...      1   \n",
       "4     103_7.txt  read comment decided watch movie first cast sp...      3   \n",
       "..          ...                                                ...    ...   \n",
       "991   997_7.txt  agree posts comedy drama leaned little much to...      1   \n",
       "992   998_7.txt  really interesting movie action movie comedy m...      1   \n",
       "993  999_10.txt  amazed movie others average 5 stars lower crap...  Other   \n",
       "994    99_8.txt  christmas together actually came time raised j...      1   \n",
       "995     9_7.txt  working class romantic drama director martin r...      1   \n",
       "\n",
       "                                            Lemma-text  \n",
       "0    [comedy, run, time, school, life, teacher, yea...  \n",
       "1    [minute, craziness, assault, psychedelic, puls...  \n",
       "2    [list, reason, source, see, include, program, ...  \n",
       "3    [first, hear, film, kid, grade, school, happen...  \n",
       "4    [read, comment, decide, watch, movie, first, c...  \n",
       "..                                                 ...  \n",
       "991  [agree, post, comedy, drama, lean, much, comed...  \n",
       "992  [really, movie, action, movie, comedy, foxx, t...  \n",
       "993  [movie, other, star, lower, movie, average, st...  \n",
       "994  [together, actually, come, time, raise, song, ...  \n",
       "995  [work, class, drama, director, ritt, come, yet...  \n",
       "\n",
       "[996 rows x 4 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filename                                                0_9.txt\n",
       "comments      bromwell high cartoon comedy ran time programs...\n",
       "Topic                                                         1\n",
       "Lemma-text    [comedy, run, time, school, life, teacher, yea...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Filename                                                1_7.txt\n",
       "comments      like adult comedy cartoons like south park nea...\n",
       "Topic                                                         1\n",
       "Lemma-text    [adult, comedy, cartoon, park, nearly, format,...\n",
       "Name: 111, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.iloc[111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('output/gsdmm.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Question 2.1 When there is no (enough) labelled corpus to train a machine learning based NLP model, we need to \n",
    "create a training text dataset as golden standard through manual annotation. Choose a text annotation \n",
    "tool to finish the following two text annotation tasks: \n",
    " \n",
    "<i> Entity  Annotation:  “Barack  Obama  was  the  44th  President  of  the  United  States.  He  was  born  in \n",
    "Hawaii and studied law at Harvard University.”  \n",
    "Annotation Results:  \n",
    "      Barack Obama PERSON \n",
    "      44th CARDINAL \n",
    "      the United States GPE \n",
    "      Hawaii GPE \n",
    "      Harvard University ORG </i>\n",
    " \n",
    "<i> Sentiment Annotation: “De Niro has the ability to make every role he portrays into acting gold. He \n",
    "gives a great performance in this film and there is a great scene where he has to take his father to a \n",
    "home for elderly people because he can't care for him anymore that will break your heart.  I will say \n",
    "you won't see much bette acting anywhere.”  \n",
    "Annotation Results: Positive </i>\n",
    " </b>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer** \n",
    "#### For this task, I used label studio to perform annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Label Studio: It is an open-source application for data annotation that supports a variety of annotation kinds, including object identification, named entity recognition, and text classification.\n",
    "- Source: https://github.com/heartexlabs/label-studio/\n",
    "- We can install label-studio in anaconda environment, by below steps:\n",
    "\n",
    "- step 1: conda create --name label-studio\n",
    "- step 2: conda activate label-studio\n",
    "- step 3: pip install label-studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>After installation you can navigate to the google chrome browser and signup for a new account.\n",
    "<i> Click on the New project on landing page.\n",
    "\n",
    "<img src=\"output/2-1/1.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> You will get below popup window and Enter the project name\n",
    "\n",
    "<img src = \"output/2-1/2.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Now Click on the Data import and import the desired text file\n",
    "\n",
    "<img src =\"output/2-1/3.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Now click on the Labeling setup and select the <b>Natural Language Processing</b> and select the <b>Named Entity Recognition</b> tab.\n",
    "\n",
    "<img src = \"output/2-1/entity.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - <i> You will be navigated to new page and Remove all the pre existing labels and <b>add desired labels</b>\n",
    " - <i> Select the Configure date as import file from the dropdown.\n",
    " - <i> After completion of two steps click the Save button.</i>\n",
    "\n",
    "<img src= \"output/2-1/4.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>After clicking on the save button, You will be redirected to the new page to labeling the text.\n",
    "<img src = \"output/2-1/new.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Select the ID for the label You will be redirected to new page for labeling.\n",
    "<b>Follow this steps for labeling:\n",
    "    Select the label and highlight the desired text and click enter</b>\n",
    "\n",
    "<img src = \"output/2-1/k.png\">\n",
    "<b>Now repeat all the steps for the remaining labels</b>\n",
    "<img src = \"output/2-1/5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repeat the same process for sentiment annotation for upto data importing**\n",
    "\n",
    "<img src =\"output/2-1/6.png\">\n",
    "\n",
    "<img src = 'output/2-1/7.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Now, select the labeling method, select the <b> Natural Language Processing </b> and select the <b> text classification </b>\n",
    "\n",
    "<img src= 'output/2-1/text-cla.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> After selecting, you will be redirected to the next page. </i> \n",
    "<i> Click on the <b>Save</b> button. </i>\n",
    "\n",
    "<img src=\"output/2-1/8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> You will be redirected to the new page, Now select the ID and Select the type of the <b>sentiment</b>\n",
    "<i> Click on the <b> submit </b> button to save the annotation </i>\n",
    "\n",
    "<img src = \"output/2-1/9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Thus, Entity and Sentiment Annotation is done by label-studio. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2-2 Active learning is a method to improve annotation efficiency. The following code imitates an active \n",
    "learning process. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Accuracy: 0.828\n",
      "\n",
      "Iteration 2:\n",
      "Accuracy: 0.834\n",
      "\n",
      "Iteration 3:\n",
      "Accuracy: 0.851\n",
      "\n",
      "Iteration 4:\n",
      "Accuracy: 0.864\n",
      "\n",
      "Iteration 5:\n",
      "Accuracy: 0.874\n",
      "\n",
      "Iteration 6:\n",
      "Accuracy: 0.879\n",
      "\n",
      "Iteration 7:\n",
      "Accuracy: 0.881\n",
      "\n",
      "Iteration 8:\n",
      "Accuracy: 0.883\n",
      "\n",
      "Iteration 9:\n",
      "Accuracy: 0.886\n",
      "\n",
      "Iteration 10:\n",
      "Accuracy: 0.894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42) \n",
    "\n",
    "# Split the dataset into initial training set and pool set \n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42) \n",
    "\n",
    "# Initialize the active learning loop \n",
    "iterations = 10 \n",
    "batch_size = 10 \n",
    "model = LogisticRegression(random_state=42) \n",
    "\n",
    "for i in range(iterations): \n",
    "    print(\"Iteration {}:\".format(i+1)) \n",
    "\n",
    "    # Train the model on the current training set \n",
    "    model.fit(X_train, y_train) \n",
    "    \n",
    "    # Predict the labels of the unlabeled instances in the pool set \n",
    "    y_pool_pred = model.predict(X_pool) \n",
    "    \n",
    "    ### below \n",
    "    y_pool_prob = model.predict_proba(X_pool) \n",
    "    entropy = -np.sum(y_pool_prob * np.log(y_pool_prob), axis=1) \n",
    "    query_idx = np.argsort(entropy)[-batch_size:] \n",
    "    ### above \n",
    "\n",
    "    X_query = X_pool[query_idx] \n",
    "    y_query = y_pool[query_idx] \n",
    "\n",
    "    # Add the labeled instances to the training set and remove them from the pool set \n",
    "    X_train = np.concatenate([X_train, X_query]) \n",
    "    y_train = np.concatenate([y_train, y_query]) \n",
    "    X_pool = np.delete(X_pool, query_idx, axis=0) \n",
    "    y_pool = np.delete(y_pool, query_idx) \n",
    "\n",
    "    # Compute and print the accuracy of the model on the test set \n",
    "    y_test_pred = model.predict(X_pool) \n",
    "    accuracy = accuracy_score(y_pool, y_test_pred) \n",
    "    print(\"Accuracy: {:.3f}\\n\".format(accuracy)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>2-2(a)</b>What is the purpose of the code between “### below” and “### above”? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Above code is Uncertainty-based sampling </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> This code calculates the entropy of the logistic regression model's predicted probabilities on the pool set's unlabeled examples, then chooses the most uncertain instances to be classified and added to the training set.\n",
    "\n",
    "Especially regarding, model.predict proba(X pool) determines the predicted probability of the logistic regression model on the unlabeled instances in the pool set. The result is a 2D array with the shape (n pool, 2), where n pool denotes the quantity of examples in the pool set and the second dimension denotes the expected probability for each class (0 and 1).\n",
    "\n",
    "For each occurrence in the pool set, the entropy of the predicted probabilities is calculated using the formula -np.sum(y pool prob * np.log(y pool prob), axis=1). The negative sum of the product of the predicted probabilities and the predicted probabilities' logarithm is known as entropy, which is a measure of uncertainty. The array's second axis, or axis=1, is used in this calculation to total the entropy values for each instance.\n",
    "\n",
    "np.argsort(entropy)[-batch size:] The indices of the instances with the highest entropy are returned after sorting the entropy values in ascending order. The instances with the highest entropy are chosen using the [-batch size:] notation's last batch size indices. The query idx variable, which is used to choose the instances to be labeled and included to the training set, stores these indices.</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Replace these code and other necessary code(as few as possible) to implement  the  active  learning  method  in  another \n",
    "strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> I'm chossing the Query-by-committe Sampling </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Query-by-committee: generates a committee of hypotheses and\n",
    "selects the unlabeled examples on the basis of disagreement among\n",
    "different hypotheses.</b>\n",
    "- <b> Suppose we create several distinct models with the same dataset. One model can be of SVM, the second model can be of Decision Tree, third can be of Logistic Regression, and so on...\n",
    "For this code, I'm using Logistic Regression, Random Forest, \n",
    "- <b> Now among this committee of different models, we measure disagreement in the predictions for a particular data sample.\n",
    "- <b> Active learner determines to query the annotator for labeling a data sample if it produces most disagreement in terms of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Accuracy: 0.871\n",
      "\n",
      "Iteration 2:\n",
      "Accuracy: 0.878\n",
      "\n",
      "Iteration 3:\n",
      "Accuracy: 0.885\n",
      "\n",
      "Iteration 4:\n",
      "Accuracy: 0.888\n",
      "\n",
      "Iteration 5:\n",
      "Accuracy: 0.880\n",
      "\n",
      "Iteration 6:\n",
      "Accuracy: 0.888\n",
      "\n",
      "Iteration 7:\n",
      "Accuracy: 0.895\n",
      "\n",
      "Iteration 8:\n",
      "Accuracy: 0.904\n",
      "\n",
      "Iteration 9:\n",
      "Accuracy: 0.911\n",
      "\n",
      "Iteration 10:\n",
      "Accuracy: 0.910\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import make_classification \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42) \n",
    "\n",
    "X_train, X_pool, y_train, y_pool = train_test_split(X, y, test_size=0.9, random_state=42) \n",
    "\n",
    "iterations = 10 \n",
    "batch_size = 10 \n",
    "\n",
    "classifiers = [LogisticRegression(random_state=42), RandomForestClassifier(random_state=42)]   \n",
    "\n",
    "for i in range(iterations): \n",
    "    print(\"Iteration {}:\".format(i+1)) \n",
    "    \n",
    "    committee_pred = np.zeros((X_pool.shape[0], len(classifiers)))\n",
    "    for j, clf in enumerate(classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        committee_pred[:, j] = clf.predict(X_pool)\n",
    "    \n",
    "    # Compute disagreement scores\n",
    "    disagreement = np.sum(committee_pred != np.expand_dims(committee_pred.mean(axis=1), axis=1), axis=1)\n",
    "    \n",
    "    # Select the most uncertain instances to be labeled and added to the training set\n",
    "    query_idx = np.argsort(disagreement)[-batch_size:]\n",
    "    \n",
    "    X_query = X_pool[query_idx] \n",
    "    y_query = y_pool[query_idx] \n",
    "\n",
    "    X_train = np.concatenate([X_train, X_query]) \n",
    "    y_train = np.concatenate([y_train, y_query]) \n",
    "    X_pool = np.delete(X_pool, query_idx, axis=0) \n",
    "    y_pool = np.delete(y_pool, query_idx) \n",
    "\n",
    "    committee_pred = np.zeros((X_pool.shape[0], len(classifiers)))\n",
    "    for j, clf in enumerate(classifiers):\n",
    "        clf.fit(X_train, y_train)\n",
    "        committee_pred[:, j] = clf.predict(X_pool)\n",
    "    y_test_pred = np.round(committee_pred.mean(axis=1))\n",
    "    accuracy = accuracy_score(y_pool, y_test_pred) \n",
    "    print(\"Accuracy: {:.3f}\\n\".format(accuracy)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> Compare these two strategies, which one is better in this example?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> You can cleary see that accuracy is increasing when no. of iterations increases. Thus Query-By-Committee is better strategy compared to Uncertainty-based sampling</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><b>2-2(b)</b>If the code is used for movie review annotation, how many reviews need to be labelled by the annotator every time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The annotator would have to label 10 reviews per iteration if the code were used to annotate 1000 comments for movie reviews. This is due to the code's use of a batch size of 10, which divides each iteration into batches of 10 comments. In order to label 10 comments in one iteration, the annotator would have to name one comment every batch.\n",
    "\n",
    "This is only a rough estimate, though. Depending on the caliber of the annotations, the expertise of the annotator, and the required level of model accuracy, the actual number of reviews that need to be labeled may change.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Discuss the possible pros and cons by increasing and decreasing this number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The model's accuracy might be increased by increasing the number of reviews that must be labeled in a single iteration. This is because the model would be able to learn from a wider variety of comments and would have access to more data to train on. The annotator would need additional time to classify the remarks, though.\n",
    "\n",
    "The annotator might save time by decreasing the amount of reviews that need to be labeled throughout each iteration. Yet it can also make the model less accurate. This is because the model would be less able to learn from a wide range of comments and would have fewer data to train on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
