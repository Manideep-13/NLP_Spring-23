{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, nltk, spacy, gensim\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lda\n",
    "import lda\n",
    "import lda.datasets\n",
    "\n",
    "# Gsdmm\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    folder_path = ['comments1k/0_9.txt', 'comments1k/1_7.txt']\n",
    "\n",
    "    # create an empty list to store the data\n",
    "    data = []\n",
    "\n",
    "    # loop through all files in the folder\n",
    "    for filename in folder_path:\n",
    "            # read the file\n",
    "            with open(filename, \"r\") as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # append the data to the list\n",
    "            data.append({\"data\": content})\n",
    "\n",
    "    # create a DataFrame from the data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # display the DataFrame\n",
    "    df.head()\n",
    "    df['data'] = df['data'].str.replace('&\\w+;','')\n",
    "    df['data'] = df['data'].apply(lambda x: re.sub('<.*?>', '', x))\n",
    "\n",
    "    df['data'] = df['data'].str.lower()\n",
    "\n",
    "    df['data'] = df['data'].str.replace('[^\\w\\s]',' ')\n",
    "    # Load the stop words from NLTK\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    df['data'] = df['data'].apply(lambda x:' '.join([w for w in x.split() if w not in stop_words]))\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saima_x4lzx52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda(df):\n",
    "    import lda.datasets\n",
    "    X = lda.datasets.load_reuters()\n",
    "    vocab = lda.datasets.load_reuters_vocab()\n",
    "    titles = df['data'].tolist()\n",
    "    model = lda.LDA(n_topics=10, n_iter=1500, random_state=1)\n",
    "    model.fit(X) # model.fit_transform(X) is also available\n",
    "    topic_word = model.components_ # model.components_ also works\n",
    "    n_top_words = 8\n",
    "    for i, topic_dist in enumerate(topic_word):\n",
    "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "        print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "    doc_topic = model.doc_topic_\n",
    "    for i in range(2):\n",
    "        print(\"{} (top topic: {})\".format(titles[i], doc_topic[i].argmax()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.1 - Use  Latent  Dirichlet  Allocation  (LDA)  method  to  discover  latent  topics  in  the  dataset  with  the number  of  topics  as  10.  Output  the  top  8  words  for  each  topic.  For  the  document  “0_9.txt”  and “1_7.txt”, what topics are assigned to them? Do they make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saima_x4lzx52\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "INFO:lda:n_documents: 395\n",
      "INFO:lda:vocab_size: 4258\n",
      "INFO:lda:n_words: 84010\n",
      "INFO:lda:n_topics: 10\n",
      "INFO:lda:n_iter: 1500\n",
      "INFO:lda:<0> log likelihood: -958627\n",
      "INFO:lda:<10> log likelihood: -718617\n",
      "INFO:lda:<20> log likelihood: -699619\n",
      "INFO:lda:<30> log likelihood: -691378\n",
      "INFO:lda:<40> log likelihood: -685783\n",
      "INFO:lda:<50> log likelihood: -682819\n",
      "INFO:lda:<60> log likelihood: -680314\n",
      "INFO:lda:<70> log likelihood: -678535\n",
      "INFO:lda:<80> log likelihood: -676979\n",
      "INFO:lda:<90> log likelihood: -676090\n",
      "INFO:lda:<100> log likelihood: -675970\n",
      "INFO:lda:<110> log likelihood: -674583\n",
      "INFO:lda:<120> log likelihood: -674216\n",
      "INFO:lda:<130> log likelihood: -673307\n",
      "INFO:lda:<140> log likelihood: -672442\n",
      "INFO:lda:<150> log likelihood: -671950\n",
      "INFO:lda:<160> log likelihood: -670999\n",
      "INFO:lda:<170> log likelihood: -670513\n",
      "INFO:lda:<180> log likelihood: -669993\n",
      "INFO:lda:<190> log likelihood: -669651\n",
      "INFO:lda:<200> log likelihood: -669865\n",
      "INFO:lda:<210> log likelihood: -669257\n",
      "INFO:lda:<220> log likelihood: -669760\n",
      "INFO:lda:<230> log likelihood: -669455\n",
      "INFO:lda:<240> log likelihood: -668785\n",
      "INFO:lda:<250> log likelihood: -668460\n",
      "INFO:lda:<260> log likelihood: -668182\n",
      "INFO:lda:<270> log likelihood: -668031\n"
     ]
    }
   ],
   "source": [
    "df = preprocessing()\n",
    "X = lda(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Answer:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Topic 0: Crime (police, Miami, Versace, Cunanan)\n",
    "- Topic 1: Music (Elvis, film, music, fans, festival, concert)\n",
    "- Topic 2: Politics (Yeltsin, president, Russian, political, Russia, minister, Kremlin)\n",
    "- Topic 3: Art and Culture (city, million, century, art, exhibition, museum, cultural, sale)\n",
    "- Topic 4: Royalty (Charles, prince, king, Diana, royal, queen, family, Parker)\n",
    "- Topic 5: War and Politics (against, Germany, Catholic, French, government, war, German, rights)\n",
    "- Topic 6: Education (school, teachers, profession, students, student, whole situation, schools, episode)\n",
    "- Topic 7: Religion (Pope, Mother Teresa, Vatican, order, hospital, doctors, Catholic)\n",
    "- Topic 8: Politics and Diplomacy (Harriman, US, Clinton, Churchill, ambassador, president, east, Paris)\n",
    "- Topic 9: Death and Religion (died, church, former, Bernardin, death, funeral, life, Simpson)\n",
    "\n",
    "**Using the top words \"high,\" \"comedy,\" \"cartoon,\" \"teachers,\" and \"students,\" the model allocated it to topic 4. The material appears to be a review or summary of the animated comedy television series \"Bromwell High,\" which centers on a high school and its faculty.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.2: Because  of  the  data  sparsity,  short  text  may  not  provide  enough  context  to  adequately  inform  topic modeling.  Try  Biterm,  GSDMM  or  other  short  text  topic  model  for  our  dataset.  Compare  the  topic \n",
    "modelling results with LDA, any improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gensim.corpora.dictionary:adding document #0 to Dictionary<0 unique tokens: []>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m docs \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mto_numpy()\n\u001b[0;32m      3\u001b[0m \u001b[39m# create dictionary of all words in all documents\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dictionary \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mcorpora\u001b[39m.\u001b[39;49mDictionary(docs)\n\u001b[0;32m      6\u001b[0m \u001b[39m# filter extreme cases out of dictionary\u001b[39;00m\n\u001b[0;32m      7\u001b[0m dictionary\u001b[39m.\u001b[39mfilter_extremes(no_below\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, no_above\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, keep_n\u001b[39m=\u001b[39m\u001b[39m100000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gensim\\corpora\\dictionary.py:78\u001b[0m, in \u001b[0;36mDictionary.__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_nnz \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[39mif\u001b[39;00m documents \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_documents(documents, prune_at\u001b[39m=\u001b[39;49mprune_at)\n\u001b[0;32m     79\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_lifecycle_event(\n\u001b[0;32m     80\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcreated\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m         msg\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs\u001b[39m}\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos\u001b[39m}\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gensim\\corpora\\dictionary.py:204\u001b[0m, in \u001b[0;36mDictionary.add_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    201\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39madding document #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, docno, \u001b[39mself\u001b[39m)\n\u001b[0;32m    203\u001b[0m     \u001b[39m# update Dictionary with the document\u001b[39;00m\n\u001b[1;32m--> 204\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdoc2bow(document, allow_update\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)  \u001b[39m# ignore the result, here we only care about updating token ids\u001b[39;00m\n\u001b[0;32m    206\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mbuilt \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m from \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m documents (total \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m corpus positions)\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_docs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_pos)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\gensim\\corpora\\dictionary.py:241\u001b[0m, in \u001b[0;36mDictionary.doc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert `document` into the bag-of-words (BoW) format = list of `(token_id, token_count)` tuples.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \n\u001b[0;32m    211\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(document, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 241\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdoc2bow expects an array of unicode tokens on input, not a single string\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m \u001b[39m# Construct (word, frequency) mapping.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m counter \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "docs = df.data.to_numpy()\n",
    "\n",
    "# create dictionary of all words in all documents\n",
    "dictionary = gensim.corpora.Dictionary(docs)\n",
    "\n",
    "# filter extreme cases out of dictionary\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "# create variable containing length of dictionary/vocab\n",
    "vocab_length = len(dictionary)\n",
    "\n",
    "# create BOW dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "# initialize GSDMM\n",
    "gsdmm = MovieGroupProcess(K=15, alpha=0.1, beta=0.3, n_iters=15)\n",
    "\n",
    "# fit GSDMM model\n",
    "y = gsdmm.fit(docs, vocab_length)\n",
    "\n",
    "doc_count = np.array(gsdmm.cluster_doc_count)\n",
    "print('Number of documents per topic :', doc_count)\n",
    "\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-15:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "\n",
    "# define function to get top words per topic\n",
    "def top_words(cluster_word_distribution, top_cluster, values):\n",
    "    for cluster in top_cluster:\n",
    "        sort_dicts = sorted(cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n",
    "        print(\"\\nCluster %s : %s\"%(cluster, sort_dicts))\n",
    "\n",
    "# get top words in topics\n",
    "top_words(gsdmm.cluster_word_distribution, top_index, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
