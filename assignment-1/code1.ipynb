{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import spacy as sp\n",
    "import string\n",
    "import os\n",
    "import en_core_web_sm\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import urllib.request\n",
    "nlp = sp.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepossessing(comments):\n",
    "    processed_comments = []\n",
    "    totalSentences = 0\n",
    "    totalTokens = 0\n",
    "    totalWords = 0\n",
    "    numComments = len(comments)\n",
    "    \n",
    "    for comment in comments:\n",
    "        # Process the comment using spaCy NLP model\n",
    "        doc = nlp(comment)\n",
    "        \n",
    "        # Count the number of sentences\n",
    "        totalSentences += len(list(doc.sents))\n",
    "        \n",
    "        # Count the number of tokens\n",
    "        totalTokens += len(doc)\n",
    "        \n",
    "        # Count the number of words without considering punctuation\n",
    "        tokens = comment.split()\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "        totalWords += len(tokens)   \n",
    "        \n",
    "        processed_comments.append(doc)\n",
    "        \n",
    "    avgSentences = totalSentences / len(comments)\n",
    "    avgTokens = totalTokens / len(comments)\n",
    "    avgWords = totalWords / len(comments)\n",
    "    \n",
    "    print(\"Average number of sentences per comment:\", avgSentences)\n",
    "    print(\"Average number of tokens per comment:\", avgTokens)\n",
    "    print(\"Average number of words per comment (without considering punctuation):\", avgWords)\n",
    "    return avgSentences,avgTokens,avgWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(i,words):\n",
    "    startTime = time.time()\n",
    "    model = Word2Vec(words, vector_size=i,window = 5, min_count = 1,workers=4)\n",
    "    endTime = time.time()\n",
    "    print('Time used for training',endTime-startTime,'Seconds')\n",
    "    print('Computer Parameters: ')\n",
    "    print(f\"- CPU: {os.cpu_count()} cores\") \n",
    "    print(\"Vocabulary size:\", len(model.wv.index_to_key))\n",
    "    if(i == 1):\n",
    "        model.save('model/word2vec1.model')\n",
    "    if(i==10):\n",
    "        model.save('model/word2vec10.model')\n",
    "    if(i==100):\n",
    "        model.save('model/word2vec100.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_word2vec(i,word):\n",
    "    if(i == 1):\n",
    "        model = Word2Vec.load('model/word2vec1.model')\n",
    "    if(i == 10):\n",
    "        model = Word2Vec.load('model/word2vec10.model')\n",
    "    if(i == 100):\n",
    "        model = Word2Vec.load('model/word2vec100.model') \n",
    "    vector = model.wv[word]\n",
    "    similar_words = model.wv.most_similar(word,topn = 10)\n",
    "    print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_GloVe():\n",
    "    #pretrained GloVe model\n",
    "    glove_file = 'glove/vectors.txt'\n",
    "    tmp_file = \"glove_To_word2vec.txt\"\n",
    "    #Convert word2vec model to GloVe model\n",
    "    f = glove2word2vec(glove_file, tmp_file)\n",
    "    model = KeyedVectors.load_word2vec_format(tmp_file)\n",
    "    model.save('model/glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_glove(word):\n",
    "\tmodel = KeyedVectors.load('model/glove.model')\n",
    "\tvector = model[word]\n",
    "\tsimilar_words = model.most_similar(word,topn = 10)\n",
    "\tprint(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saima\\AppData\\Local\\Temp\\ipykernel_13404\\905427216.py:9: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1[0] = df1[0].str.replace('&\\w+;','')\n",
      "C:\\Users\\saima\\AppData\\Local\\Temp\\ipykernel_13404\\905427216.py:14: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1[0] = df1[0].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "directory = \"comments1k\"\n",
    "comments = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(os.path.join(directory, filename)) as file:\n",
    "            comments.append(file.read().strip())\n",
    "df1 = pd.DataFrame(comments)\n",
    "\n",
    "df1[0] = df1[0].str.replace('&\\w+;','')\n",
    "df1[0] = df1[0].apply(lambda x: re.sub('<.*?>', '', x))\n",
    "\n",
    "df1[0] = df1[0].str.lower()\n",
    "\n",
    "df1[0] = df1[0].str.replace('[^\\w\\s]','')\n",
    "\n",
    "df = [row.split(' ') for row in df1[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bromwell high is a cartoon comedy it ran at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>scott bartletts offon is nine minutes of pure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>imdb lists this as 1972 for some reason but th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i first heard about this film about 20 years a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when i read other commenti decided to watch th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>i have to agree with most of the other posts w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>this is a really interesting movie it is an ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>i am amazed at how this movieand most others h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>a christmas together actually came before my t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>workingclass romantic drama from director mart...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0    bromwell high is a cartoon comedy it ran at th...\n",
       "1    scott bartletts offon is nine minutes of pure ...\n",
       "2    imdb lists this as 1972 for some reason but th...\n",
       "3    i first heard about this film about 20 years a...\n",
       "4    when i read other commenti decided to watch th...\n",
       "..                                                 ...\n",
       "991  i have to agree with most of the other posts w...\n",
       "992  this is a really interesting movie it is an ac...\n",
       "993  i am amazed at how this movieand most others h...\n",
       "994  a christmas together actually came before my t...\n",
       "995  workingclass romantic drama from director mart...\n",
       "\n",
       "[996 rows x 1 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of sentences per comment: 13.301204819277109\n",
      "Average number of tokens per comment: 272.5301204819277\n",
      "Average number of words per comment (without considering punctuation): 234.26706827309238\n",
      "word2vec method with CBOW model and vector size as 100\n",
      "Time used for training 0.9705944061279297 Seconds\n",
      "Computer Parameters: \n",
      "- CPU: 8 cores\n",
      "Vocabulary size: 19272\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of movie :\n",
      "[('film', 0.9733423590660095), ('doubt', 0.9663307666778564), ('thing', 0.9636635184288025), ('saw', 0.9636475443840027), ('liked', 0.962794303894043), ('recommend', 0.9621709585189819), ('thought', 0.9605566263198853), ('comment', 0.9585217833518982), ('felt', 0.9579874277114868), ('bought', 0.9572857618331909)]\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of music :\n",
      "[('war', 0.9993916749954224), ('character', 0.999042272567749), ('novel', 0.9990364909172058), ('supporting', 0.9990068078041077), ('picture', 0.9989712834358215), ('jedi', 0.9988833069801331), ('beautiful', 0.998854398727417), ('beauty', 0.9988070130348206), ('wonderful', 0.9987823367118835), ('early', 0.9987727403640747)]\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of woman :\n",
      "[('man', 0.9994759559631348), ('young', 0.9991185665130615), ('whose', 0.9990518093109131), ('white', 0.9990038871765137), ('paul', 0.9989868402481079), ('leading', 0.9988653659820557), ('plays', 0.998810350894928), ('greg', 0.9988047480583191), ('gold', 0.9987991452217102), ('ben', 0.9987350106239319)]\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of Christmas :\n",
      "[('last', 0.9993923902511597), ('day', 0.9992798566818237), ('among', 0.9992411732673645), ('piece', 0.999104917049408), ('fantasy', 0.9990890026092529), ('type', 0.999077558517456), ('star', 0.999021053314209), ('horror', 0.9990026950836182), ('tale', 0.9989903569221497), ('making', 0.9989741444587708)]\n",
      "\n",
      "\n",
      "Glove model and vector size as 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saima\\AppData\\Local\\Temp\\ipykernel_13404\\1833267865.py:6: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  f = glove2word2vec(glove_file, tmp_file)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  trained  in glove by  analyzing  the  10  most  similar  words of movie :\n",
      "[('this', 0.8925120830535889), ('film', 0.8462427854537964), ('it', 0.7992120385169983), ('made', 0.7556362152099609), ('actually', 0.7472920417785645), ('think', 0.7353578209877014), ('just', 0.7298932075500488), ('really', 0.7049843668937683), ('one', 0.7032972574234009), ('but', 0.6959174871444702)]\n",
      "model  trained  in glove by  analyzing  the  10  most  similar  words of music :\n",
      "[('dialogue', 0.772396981716156), ('great', 0.7340028882026672), ('effects', 0.722646176815033), ('video', 0.7005497813224792), ('cinematography', 0.6866236329078674), ('special', 0.6834750771522522), ('scenery', 0.6818291544914246), ('casting', 0.6797624826431274), ('touching', 0.677040696144104), ('dramatic', 0.6764266490936279)]\n",
      "model  trained  in glove by  analyzing  the  10  most  similar  words of woman :\n",
      "[('girl', 0.831171989440918), ('named', 0.8100281953811646), ('young', 0.8018996119499207), ('man', 0.7678036093711853), ('africanamerican', 0.7610023617744446), ('boy', 0.7065350413322449), ('plays', 0.6875846982002258), ('becomes', 0.6874582171440125), ('who', 0.6838185787200928), ('whose', 0.6792166829109192)]\n",
      "model  trained  in glove by  analyzing  the  10  most  similar  words of Christmas :\n",
      "[('carol', 0.9032812118530273), ('present', 0.884943425655365), ('eve', 0.7939966917037964), ('classic', 0.6476872563362122), ('greatest', 0.6467152237892151), ('100', 0.6414575576782227), ('giallo', 0.6320691704750061), ('favourite', 0.6258476376533508), ('series', 0.6124653220176697), ('complete', 0.6042324304580688)]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    prepossessing(comments)\n",
    "    print('word2vec method with CBOW model and vector size as 100')\n",
    "    train_word2vec(100,df)\n",
    "    print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of movie :')\n",
    "    evaluate_word2vec(100,'movie')\n",
    "    print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of music :')\n",
    "    evaluate_word2vec(100,'music')\n",
    "    print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of woman :')\n",
    "    evaluate_word2vec(100,'woman')\n",
    "    print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of Christmas :')\n",
    "    evaluate_word2vec(100,'christmas')\n",
    "    print('\\n')\n",
    "    print('Glove model and vector size as 100')\n",
    "    train_GloVe()\n",
    "    print('model  trained  in glove by  analyzing  the  10  most  similar  words of movie :')\n",
    "    evaluate_glove('movie')\n",
    "    print('model  trained  in glove by  analyzing  the  10  most  similar  words of music :')\n",
    "    evaluate_glove('music')\n",
    "    print('model  trained  in glove by  analyzing  the  10  most  similar  words of woman :')\n",
    "    evaluate_glove('woman')\n",
    "    print('model  trained  in glove by  analyzing  the  10  most  similar  words of Christmas :')\n",
    "    evaluate_glove('christmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec method with CBOW model and vector size as 1\n",
      "Time used for training 1.5334162712097168 Seconds\n",
      "Computer Parameters: \n",
      "- CPU: 8 cores\n",
      "Vocabulary size: 19272\n"
     ]
    }
   ],
   "source": [
    "print('word2vec method with CBOW model and vector size as 1')\n",
    "train_word2vec(1,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of movie :\n",
      "[('missions', 1.0), ('wholeheartedlywell', 1.0), ('jugular', 1.0), ('sideline', 1.0), ('stara', 1.0), ('ingÃ£nue', 1.0), ('garbo', 1.0), ('onereel', 1.0), ('manshow', 1.0), ('collaborate', 1.0)]\n",
      "model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of music :\n",
      "[('missions', 1.0), ('wholeheartedlywell', 1.0), ('jugular', 1.0), ('sideline', 1.0), ('stara', 1.0), ('ingÃ£nue', 1.0), ('garbo', 1.0), ('onereel', 1.0), ('manshow', 1.0), ('collaborate', 1.0)]\n",
      "model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of woman :\n",
      "[('missions', 1.0), ('wholeheartedlywell', 1.0), ('jugular', 1.0), ('sideline', 1.0), ('stara', 1.0), ('ingÃ£nue', 1.0), ('garbo', 1.0), ('onereel', 1.0), ('manshow', 1.0), ('collaborate', 1.0)]\n",
      "model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of Christmas :\n",
      "[('missions', 1.0), ('wholeheartedlywell', 1.0), ('jugular', 1.0), ('sideline', 1.0), ('stara', 1.0), ('ingÃ£nue', 1.0), ('garbo', 1.0), ('onereel', 1.0), ('manshow', 1.0), ('collaborate', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print('model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of movie :')\n",
    "evaluate_word2vec(1,'movie')\n",
    "print('model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of music :')\n",
    "evaluate_word2vec(1,'music')\n",
    "print('model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of woman :')\n",
    "evaluate_word2vec(1,'woman')\n",
    "print('model  trained  in word2vec of 1 vector size by  analyzing  the  10  most  similar  words of Christmas :')\n",
    "evaluate_word2vec(1,'christmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word2vec method with CBOW model and vector size as 10\n",
      "Time used for training 1.5839741230010986 Seconds\n",
      "Computer Parameters: \n",
      "- CPU: 8 cores\n",
      "Vocabulary size: 19272\n"
     ]
    }
   ],
   "source": [
    "print('word2vec method with CBOW model and vector size as 10')\n",
    "train_word2vec(10,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of movie :\n",
      "[('film', 0.9873573780059814), ('saw', 0.9842372536659241), ('taste', 0.9759257435798645), ('tape', 0.9733883738517761), ('thing', 0.9716353416442871), ('sit', 0.9695032238960266), ('loved', 0.9674909710884094), ('recommend', 0.965630829334259), ('overthetop', 0.9645233750343323), ('crashing', 0.9614243507385254)]\n",
      "model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of music :\n",
      "[('direction', 0.9991080164909363), ('killer', 0.9979554414749146), ('themes', 0.997456431388855), ('magic', 0.9974030256271362), ('clever', 0.9973888993263245), ('action', 0.9973253011703491), ('excellent', 0.9972029328346252), ('especially', 0.996849536895752), ('place', 0.9968379735946655), ('late', 0.9966833591461182)]\n",
      "model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of woman :\n",
      "[('young', 0.9986621141433716), ('boy', 0.9973291158676147), ('plays', 0.997189462184906), ('man', 0.9968857765197754), ('soles', 0.9964021444320679), ('mother', 0.9959387183189392), ('becomes', 0.9957179427146912), ('between', 0.9956828951835632), ('character', 0.9956547617912292), ('land', 0.9955892562866211)]\n",
      "model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of Christmas :\n",
      "[('us', 0.9984123110771179), ('spanish', 0.9978294968605042), ('place', 0.9977760910987854), ('several', 0.9977359175682068), ('episode', 0.9975878000259399), ('hollywood', 0.9975776076316833), ('high', 0.9972559213638306), ('day', 0.9968909025192261), ('late', 0.9966220855712891), ('both', 0.9965990781784058)]\n"
     ]
    }
   ],
   "source": [
    "print('model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of movie :')\n",
    "evaluate_word2vec(10,'movie')\n",
    "print('model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of music :')\n",
    "evaluate_word2vec(10,'music')\n",
    "print('model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of woman :')\n",
    "evaluate_word2vec(10,'woman')\n",
    "print('model  trained  in word2vec of 10 vector size by  analyzing  the  10  most  similar  words of Christmas :')\n",
    "evaluate_word2vec(10,'christmas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of movie :\n",
      "[('film', 0.9733423590660095), ('doubt', 0.9663307666778564), ('thing', 0.9636635184288025), ('saw', 0.9636475443840027), ('liked', 0.962794303894043), ('recommend', 0.9621709585189819), ('thought', 0.9605566263198853), ('comment', 0.9585217833518982), ('felt', 0.9579874277114868), ('bought', 0.9572857618331909)]\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of music :\n",
      "[('war', 0.9993916749954224), ('character', 0.999042272567749), ('novel', 0.9990364909172058), ('supporting', 0.9990068078041077), ('picture', 0.9989712834358215), ('jedi', 0.9988833069801331), ('beautiful', 0.998854398727417), ('beauty', 0.9988070130348206), ('wonderful', 0.9987823367118835), ('early', 0.9987727403640747)]\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of woman :\n",
      "[('man', 0.9994759559631348), ('young', 0.9991185665130615), ('whose', 0.9990518093109131), ('white', 0.9990038871765137), ('paul', 0.9989868402481079), ('leading', 0.9988653659820557), ('plays', 0.998810350894928), ('greg', 0.9988047480583191), ('gold', 0.9987991452217102), ('ben', 0.9987350106239319)]\n",
      "model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of Christmas :\n",
      "[('last', 0.9993923902511597), ('day', 0.9992798566818237), ('among', 0.9992411732673645), ('piece', 0.999104917049408), ('fantasy', 0.9990890026092529), ('type', 0.999077558517456), ('star', 0.999021053314209), ('horror', 0.9990026950836182), ('tale', 0.9989903569221497), ('making', 0.9989741444587708)]\n"
     ]
    }
   ],
   "source": [
    "print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of movie :')\n",
    "evaluate_word2vec(100,'movie')\n",
    "print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of music :')\n",
    "evaluate_word2vec(100,'music')\n",
    "print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of woman :')\n",
    "evaluate_word2vec(100,'woman')\n",
    "print('model  trained  in word2vec of 100 vector size by  analyzing  the  10  most  similar  words of Christmas :')\n",
    "evaluate_word2vec(100,'christmas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
